{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Complex Transformations\n",
    "\n",
    "This notebook demonstrates advanced feature engineering using PySpark functions on the processed YouTube Analytics data.\n",
    "\n",
    "## Tasks:\n",
    "1. **engagement_score**: Weighted metric combining likes, dislikes, and comments relative to views\n",
    "2. **days_to_trend**: Calculate days between trending_date and publish_time\n",
    "3. **trending_rank**: Rank videos within each trending_date and category using Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup notebook environment\n",
    "from notebook_setup import setup_notebook_environment, test_imports\n",
    "\n",
    "# Setup paths and test imports\n",
    "project_root = setup_notebook_environment()\n",
    "test_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from config.settings import Config\n",
    "from src.utils.spark_utils import SparkUtils\n",
    "from src.data_ingestion.processed_data_loader import ProcessedDataLoader\n",
    "\n",
    "# PySpark functions for feature engineering\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkUtils.get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Initialize data loader\n",
    "loader = ProcessedDataLoader(spark)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "Load the processed YouTube trending videos data that was generated by our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed video data\n",
    "print(\"Loading processed YouTube trending videos data...\")\n",
    "df = loader.load_processed_videos()\n",
    "\n",
    "if df is None:\n",
    "    raise ValueError(\"Could not load processed data. Please run the pipeline first.\")\n",
    "\n",
    "print(f\"Loaded {df.count():,} records\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample of loaded data:\")\n",
    "df.select(\n",
    "    \"video_id\", \"title\", \"channel_title\", \"trending_date\", \"publish_time\",\n",
    "    \"views\", \"likes\", \"dislikes\", \"comment_count\", \"category_name\", \"country\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Calculate Engagement Score\n",
    "\n",
    "Create an `engagement_score` column using the formula:\n",
    "```\n",
    "engagement_score = ((likes * 0.5) + (dislikes * 0.2) + (comment_count * 0.3)) / views\n",
    "```\n",
    "\n",
    "We'll handle division-by-zero errors by setting engagement_score to 0 when views is 0 or null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Calculate engagement_score\n",
    "print(\"Task 1: Calculating engagement_score...\")\n",
    "\n",
    "# Calculate engagement score with division-by-zero handling\n",
    "df_with_engagement = df.withColumn(\n",
    "    \"engagement_score\",\n",
    "    F.when(\n",
    "        (F.col(\"views\").isNull()) | (F.col(\"views\") == 0),\n",
    "        0.0\n",
    "    ).otherwise(\n",
    "        (\n",
    "            (F.coalesce(F.col(\"likes\"), F.lit(0)) * 0.5) +\n",
    "            (F.coalesce(F.col(\"dislikes\"), F.lit(0)) * 0.2) +\n",
    "            (F.coalesce(F.col(\"comment_count\"), F.lit(0)) * 0.3)\n",
    "        ) / F.col(\"views\")\n",
    "    ).cast(DoubleType())\n",
    ")\n",
    "\n",
    "print(\"Engagement score calculated successfully!\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample engagement scores:\")\n",
    "df_with_engagement.select(\n",
    "    \"video_id\", \"title\", \"views\", \"likes\", \"dislikes\", \"comment_count\", \"engagement_score\"\n",
    ").orderBy(F.desc(\"engagement_score\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze engagement score statistics\n",
    "print(\"Engagement Score Statistics:\")\n",
    "engagement_stats = df_with_engagement.select(\n",
    "    F.count(\"engagement_score\").alias(\"count\"),\n",
    "    F.mean(\"engagement_score\").alias(\"mean\"),\n",
    "    F.stddev(\"engagement_score\").alias(\"stddev\"),\n",
    "    F.min(\"engagement_score\").alias(\"min\"),\n",
    "    F.max(\"engagement_score\").alias(\"max\"),\n",
    "    F.expr(\"percentile_approx(engagement_score, 0.5)\").alias(\"median\")\n",
    ").collect()[0]\n",
    "\n",
    "for field in engagement_stats.asDict():\n",
    "    value = engagement_stats[field]\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{field}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"{field}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculate Days to Trend\n",
    "\n",
    "Calculate the number of days between `trending_date` and `publish_time`.\n",
    "This requires parsing both date columns and computing the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Calculate days_to_trend\n",
    "print(\"Task 2: Calculating days_to_trend...\")\n",
    "\n",
    "# First, let's examine the date formats\n",
    "print(\"Examining date formats:\")\n",
    "df_with_engagement.select(\"trending_date\", \"publish_time\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse trending_date (format: YY.DD.MM) and publish_time (ISO format)\n",
    "df_with_dates = df_with_engagement.withColumn(\n",
    "    \"trending_date_parsed\",\n",
    "    F.to_date(F.col(\"trending_date\"), \"yy.dd.MM\")\n",
    ").withColumn(\n",
    "    \"publish_date_parsed\",\n",
    "    F.to_date(F.col(\"publish_time_parsed\"))\n",
    ")\n",
    "\n",
    "# Calculate days_to_trend\n",
    "df_with_days_to_trend = df_with_dates.withColumn(\n",
    "    \"days_to_trend\",\n",
    "    F.when(\n",
    "        F.col(\"trending_date_parsed\").isNull() | F.col(\"publish_date_parsed\").isNull(),\n",
    "        None\n",
    "    ).otherwise(\n",
    "        F.datediff(F.col(\"trending_date_parsed\"), F.col(\"publish_date_parsed\"))\n",
    "    ).cast(IntegerType())\n",
    ")\n",
    "\n",
    "print(\"Days to trend calculated successfully!\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample days_to_trend calculations:\")\n",
    "df_with_days_to_trend.select(\n",
    "    \"video_id\", \"title\", \"trending_date\", \"publish_time\", \n",
    "    \"trending_date_parsed\", \"publish_date_parsed\", \"days_to_trend\"\n",
    ").filter(F.col(\"days_to_trend\").isNotNull()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze days_to_trend statistics\n",
    "print(\"Days to Trend Statistics:\")\n",
    "days_stats = df_with_days_to_trend.select(\n",
    "    F.count(\"days_to_trend\").alias(\"count\"),\n",
    "    F.mean(\"days_to_trend\").alias(\"mean\"),\n",
    "    F.stddev(\"days_to_trend\").alias(\"stddev\"),\n",
    "    F.min(\"days_to_trend\").alias(\"min\"),\n",
    "    F.max(\"days_to_trend\").alias(\"max\"),\n",
    "    F.expr(\"percentile_approx(days_to_trend, 0.5)\").alias(\"median\")\n",
    ").collect()[0]\n",
    "\n",
    "for field in days_stats.asDict():\n",
    "    value = days_stats[field]\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{field}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{field}: {value}\")\n",
    "\n",
    "# Show distribution of days_to_trend\n",
    "print(\"\\nDays to Trend Distribution:\")\n",
    "df_with_days_to_trend.groupBy(\"days_to_trend\").count().orderBy(\"days_to_trend\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Calculate Trending Rank\n",
    "\n",
    "Use PySpark Window Functions to rank videos within each `trending_date` and `category_name` based on their `views` in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Calculate trending_rank using Window Functions\n",
    "print(\"Task 3: Calculating trending_rank using Window Functions...\")\n",
    "\n",
    "# Define window specification: partition by trending_date and category_name, order by views descending\n",
    "window_spec = Window.partitionBy(\"trending_date\", \"category_name\").orderBy(F.desc(\"views\"))\n",
    "\n",
    "# Calculate trending_rank\n",
    "df_with_rank = df_with_days_to_trend.withColumn(\n",
    "    \"trending_rank\",\n",
    "    F.row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"Trending rank calculated successfully!\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample trending ranks (Top 3 videos per category per day):\")\n",
    "df_with_rank.select(\n",
    "    \"trending_date\", \"category_name\", \"trending_rank\", \"title\", \"channel_title\", \"views\"\n",
    ").filter(F.col(\"trending_rank\") <= 3).orderBy(\n",
    "    \"trending_date\", \"category_name\", \"trending_rank\"\n",
    ").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze trending rank distribution\n",
    "print(\"Trending Rank Analysis:\")\n",
    "\n",
    "# Count videos by rank position\n",
    "print(\"\\nDistribution of trending ranks:\")\n",
    "df_with_rank.groupBy(\"trending_rank\").count().orderBy(\"trending_rank\").show(10)\n",
    "\n",
    "# Show top-ranked videos across all categories and dates\n",
    "print(\"\\nTop-ranked videos (rank = 1) by category and date:\")\n",
    "top_ranked = df_with_rank.filter(F.col(\"trending_rank\") == 1)\n",
    "print(f\"Total #1 ranked videos: {top_ranked.count()}\")\n",
    "\n",
    "top_ranked.select(\n",
    "    \"trending_date\", \"category_name\", \"title\", \"channel_title\", \"views\", \"engagement_score\"\n",
    ").orderBy(F.desc(\"views\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset with All Features\n",
    "\n",
    "Let's create the final dataset with all the engineered features and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset with all features\n",
    "print(\"Creating final dataset with all engineered features...\")\n",
    "\n",
    "# Select relevant columns for the final dataset\n",
    "final_df = df_with_rank.select(\n",
    "    \"video_id\", \"title\", \"channel_title\", \"category_name\", \"country\",\n",
    "    \"trending_date\", \"publish_time\", \"views\", \"likes\", \"dislikes\", \"comment_count\",\n",
    "    \"engagement_score\", \"days_to_trend\", \"trending_rank\"\n",
    ")\n",
    "\n",
    "print(f\"Final dataset contains {final_df.count():,} records with {len(final_df.columns)} columns\")\n",
    "\n",
    "# Show schema of final dataset\n",
    "print(\"\\nFinal dataset schema:\")\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show comprehensive sample of final dataset\n",
    "print(\"Sample of final dataset with all engineered features:\")\n",
    "final_df.orderBy(F.desc(\"engagement_score\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all engineered features\n",
    "print(\"Summary Statistics for Engineered Features:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Engagement Score\n",
    "print(\"\\n1. Engagement Score:\")\n",
    "final_df.select(\n",
    "    F.count(\"engagement_score\").alias(\"count\"),\n",
    "    F.mean(\"engagement_score\").alias(\"mean\"),\n",
    "    F.min(\"engagement_score\").alias(\"min\"),\n",
    "    F.max(\"engagement_score\").alias(\"max\")\n",
    ").show()\n",
    "\n",
    "# Days to Trend\n",
    "print(\"\\n2. Days to Trend:\")\n",
    "final_df.select(\n",
    "    F.count(\"days_to_trend\").alias(\"count\"),\n",
    "    F.mean(\"days_to_trend\").alias(\"mean\"),\n",
    "    F.min(\"days_to_trend\").alias(\"min\"),\n",
    "    F.max(\"days_to_trend\").alias(\"max\")\n",
    ").show()\n",
    "\n",
    "# Trending Rank\n",
    "print(\"\\n3. Trending Rank:\")\n",
    "final_df.select(\n",
    "    F.count(\"trending_rank\").alias(\"count\"),\n",
    "    F.mean(\"trending_rank\").alias(\"mean\"),\n",
    "    F.min(\"trending_rank\").alias(\"min\"),\n",
    "    F.max(\"trending_rank\").alias(\"max\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "SparkUtils.stop_spark_session()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
