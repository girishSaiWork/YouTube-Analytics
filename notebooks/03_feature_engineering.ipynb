{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Complex Transformations\n",
    "\n",
    "This notebook demonstrates advanced feature engineering using PySpark functions on the processed YouTube Analytics data.\n",
    "\n",
    "## Tasks:\n",
    "1. **engagement_score**: Weighted metric combining likes, dislikes, and comments relative to views\n",
    "2. **days_to_trend**: Calculate days between trending_date and publish_time\n",
    "3. **trending_rank**: Rank videos within each trending_date and category using Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project root: e:\\Study Space\\Analytics Enginerring\\Data Engineering\\Azure Databricks\\ADB_Practice\\YouTube Analytics\n",
      "✅ Added to Python path:\n",
      "   - e:\\Study Space\\Analytics Enginerring\\Data Engineering\\Azure Databricks\\ADB_Practice\\YouTube Analytics\n",
      "   - e:\\Study Space\\Analytics Enginerring\\Data Engineering\\Azure Databricks\\ADB_Practice\\YouTube Analytics\\src\n",
      "✅ Config import successful\n",
      "✅ SparkUtils import successful\n",
      "✅ YouTubeDataReader import successful\n",
      "✅ All imports working correctly!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup notebook environment\n",
    "from notebook_setup import setup_notebook_environment, test_imports\n",
    "\n",
    "# Setup paths and test imports\n",
    "project_root = setup_notebook_environment()\n",
    "test_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from config.settings import Config\n",
    "from src.utils.spark_utils import SparkUtils\n",
    "from src.data_ingestion.processed_data_loader import ProcessedDataLoader\n",
    "\n",
    "# PySpark functions for feature engineering\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.6\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkUtils.get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Initialize data loader\n",
    "loader = ProcessedDataLoader(spark)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "Load the processed YouTube trending videos data that was generated by our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_ingestion.processed_data_loader:Attempting to load using Pandas I/O (Windows compatibility)\n",
      "INFO:src.data_ingestion.processed_data_loader:Loading from pandas parquet: e:\\Study Space\\Analytics Enginerring\\Data Engineering\\Azure Databricks\\ADB_Practice\\YouTube Analytics\\data\\processed\\youtube_trending_videos.parquet\n",
      "INFO:src.utils.pandas_io:Loading parquet file from e:\\Study Space\\Analytics Enginerring\\Data Engineering\\Azure Databricks\\ADB_Practice\\YouTube Analytics\\data\\processed\\youtube_trending_videos.parquet using Pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed YouTube trending videos data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.pandas_io:Successfully loaded 40899 records from e:\\Study Space\\Analytics Enginerring\\Data Engineering\\Azure Databricks\\ADB_Practice\\YouTube Analytics\\data\\processed\\youtube_trending_videos.parquet\n",
      "INFO:src.data_ingestion.processed_data_loader:Loaded 40899 processed video records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40,899 records\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- likes: long (nullable = true)\n",
      " |-- dislikes: long (nullable = true)\n",
      " |-- comment_count: long (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: boolean (nullable = true)\n",
      " |-- ratings_disabled: boolean (nullable = true)\n",
      " |-- video_error_or_removed: boolean (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      " |-- publish_time_parsed: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load processed video data\n",
    "print(\"Loading processed YouTube trending videos data...\")\n",
    "df = loader.load_processed_videos()\n",
    "\n",
    "if df is None:\n",
    "    raise ValueError(\"Could not load processed data. Please run the pipeline first.\")\n",
    "\n",
    "print(f\"Loaded {df.count():,} records\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of loaded data:\n",
      "+-----------+---------------------------------------------------------+-------------+-------------+------------------------+-------+-----+--------+-------------+---------------+-------+\n",
      "|video_id   |title                                                    |channel_title|trending_date|publish_time            |views  |likes|dislikes|comment_count|category_name  |country|\n",
      "+-----------+---------------------------------------------------------+-------------+-------------+------------------------+-------+-----+--------+-------------+---------------+-------+\n",
      "|-2aVkGcI7ZA|Benedict Cumberbatch's Tom Holland impression is PERFECT.|BBC Radio 1  |18.27.04     |2018-04-25T12:20:45.000Z|1012527|19339|633     |520          |Music          |US     |\n",
      "|-2b4qSoMnKE|Ex-UFO program chief: We may not be alone                |CNN          |17.20.12     |2017-12-19T20:46:33.000Z|84744  |1444 |199     |1610         |News & Politics|US     |\n",
      "|-37nIo_tLnk|Christmas Day 2000                                       |vnbreyes     |17.28.12     |2009-12-15T23:26:32.000Z|3052   |2    |0       |0            |Sports         |US     |\n",
      "|-37nIo_tLnk|Christmas Day 2000                                       |vnbreyes     |17.31.12     |2009-12-15T23:26:32.000Z|3130   |2    |0       |0            |Sports         |US     |\n",
      "|-3h4Xt9No9o|Why Atlantic fish are invading the Arctic                |Vox          |18.25.04     |2018-04-23T12:11:34.000Z|309873 |7695 |279     |794          |News & Politics|US     |\n",
      "+-----------+---------------------------------------------------------+-------------+-------------+------------------------+-------+-----+--------+-------------+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data\n",
    "print(\"Sample of loaded data:\")\n",
    "df.select(\n",
    "    \"video_id\", \"title\", \"channel_title\", \"trending_date\", \"publish_time\",\n",
    "    \"views\", \"likes\", \"dislikes\", \"comment_count\", \"category_name\", \"country\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Calculate Engagement Score\n",
    "\n",
    "Create an `engagement_score` column using the formula:\n",
    "```\n",
    "engagement_score = ((likes * 0.5) + (dislikes * 0.2) + (comment_count * 0.3)) / views\n",
    "```\n",
    "\n",
    "We'll handle division-by-zero errors by setting engagement_score to 0 when views is 0 or null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Calculating engagement_score...\n",
      "Engagement score calculated successfully!\n",
      "\n",
      "Sample engagement scores:\n",
      "+-----------+-------------------------------------------------------------+-------+-------+--------+-------------+-------------------+\n",
      "|video_id   |title                                                        |views  |likes  |dislikes|comment_count|engagement_score   |\n",
      "+-----------+-------------------------------------------------------------+-------+-------+--------+-------------+-------------------+\n",
      "|LsoLEjrDogU|Bruno Mars - Finesse (Remix) [Feat. Cardi B] [Official Video]|548621 |159356 |2374    |19455        |0.1567371646364248 |\n",
      "|TyHvyGVs42U|Luis Fonsi, Demi Lovato - Échame La Culpa                    |499946 |135292 |3528    |12094        |0.14397514931612615|\n",
      "|8O_MwlZ2dEg|j-hope 'Airplane' MV                                         |5275672|1401915|6268    |158127       |0.14209549039439903|\n",
      "|UaAHl_m_ybk|5 Seconds Of Summer - Want You Back (Audio)                  |439056 |106536 |656     |20575        |0.13568132538901642|\n",
      "|9vDdx1dba6c|dodie - Secret For The Mad                                   |129130 |32755  |98      |2964         |0.133867420429025  |\n",
      "|inZzcTXYowY|Louis Tomlinson - Miss You (Official Video)                  |985998 |241679 |793     |26259        |0.13070594463680454|\n",
      "|D_6QmL6rExk|BTS (방탄소년단) 'FAKE LOVE' Official MV (Extended ver.)     |5884233|1437859|6390    |134721       |0.12926473169910166|\n",
      "|6mkLWGy6Y3k|Shawn Mendes: The Tour - Official Trailer                    |332910 |80118  |232     |5420         |0.12535339881649696|\n",
      "|8O_MwlZ2dEg|j-hope 'Airplane' MV                                         |6870836|1566591|7910    |167954       |0.12156653135077011|\n",
      "|6mkLWGy6Y3k|Shawn Mendes: The Tour - Official Trailer                    |355703 |82016  |249     |5459         |0.12003131826270794|\n",
      "+-----------+-------------------------------------------------------------+-------+-------+--------+-------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Calculate engagement_score\n",
    "print(\"Task 1: Calculating engagement_score...\")\n",
    "\n",
    "# Calculate engagement score with division-by-zero handling\n",
    "df_with_engagement = df.withColumn(\n",
    "    \"engagement_score\",\n",
    "    F.when(\n",
    "        (F.col(\"views\").isNull()) | (F.col(\"views\") == 0),\n",
    "        0.0\n",
    "    ).otherwise(\n",
    "        (\n",
    "            (F.coalesce(F.col(\"likes\"), F.lit(0)) * 0.5) +\n",
    "            (F.coalesce(F.col(\"dislikes\"), F.lit(0)) * 0.2) +\n",
    "            (F.coalesce(F.col(\"comment_count\"), F.lit(0)) * 0.3)\n",
    "        ) / F.col(\"views\")\n",
    "    ).cast(DoubleType())\n",
    ")\n",
    "\n",
    "print(\"Engagement score calculated successfully!\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample engagement scores:\")\n",
    "df_with_engagement.select(\n",
    "    \"video_id\", \"title\", \"views\", \"likes\", \"dislikes\", \"comment_count\", \"engagement_score\"\n",
    ").orderBy(F.desc(\"engagement_score\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engagement Score Statistics:\n",
      "count: 40899\n",
      "mean: 0.018867\n",
      "stddev: 0.014349\n",
      "min: 0.000000\n",
      "max: 0.156737\n",
      "median: 0.015630\n"
     ]
    }
   ],
   "source": [
    "# Analyze engagement score statistics\n",
    "print(\"Engagement Score Statistics:\")\n",
    "engagement_stats = df_with_engagement.select(\n",
    "    F.count(\"engagement_score\").alias(\"count\"),\n",
    "    F.mean(\"engagement_score\").alias(\"mean\"),\n",
    "    F.stddev(\"engagement_score\").alias(\"stddev\"),\n",
    "    F.min(\"engagement_score\").alias(\"min\"),\n",
    "    F.max(\"engagement_score\").alias(\"max\"),\n",
    "    F.expr(\"percentile_approx(engagement_score, 0.5)\").alias(\"median\")\n",
    ").collect()[0]\n",
    "\n",
    "for field in engagement_stats.asDict():\n",
    "    value = engagement_stats[field]\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{field}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"{field}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculate Days to Trend\n",
    "\n",
    "Calculate the number of days between `trending_date` and `publish_time`.\n",
    "This requires parsing both date columns and computing the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Calculating days_to_trend...\n",
      "Examining date formats:\n",
      "+-------------+------------------------+\n",
      "|trending_date|publish_time            |\n",
      "+-------------+------------------------+\n",
      "|18.27.04     |2018-04-25T12:20:45.000Z|\n",
      "|17.20.12     |2017-12-19T20:46:33.000Z|\n",
      "|17.28.12     |2009-12-15T23:26:32.000Z|\n",
      "|17.31.12     |2009-12-15T23:26:32.000Z|\n",
      "|18.25.04     |2018-04-23T12:11:34.000Z|\n",
      "+-------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Calculate days_to_trend\n",
    "print(\"Task 2: Calculating days_to_trend...\")\n",
    "\n",
    "# First, let's examine the date formats\n",
    "print(\"Examining date formats:\")\n",
    "df_with_engagement.select(\"trending_date\", \"publish_time\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days to trend calculated successfully!\n",
      "\n",
      "Sample days_to_trend calculations:\n",
      "+-----------+---------------------------------------------------------+-------------+------------------------+--------------------+-------------------+-------------+\n",
      "|video_id   |title                                                    |trending_date|publish_time            |trending_date_parsed|publish_date_parsed|days_to_trend|\n",
      "+-----------+---------------------------------------------------------+-------------+------------------------+--------------------+-------------------+-------------+\n",
      "|-2aVkGcI7ZA|Benedict Cumberbatch's Tom Holland impression is PERFECT.|18.27.04     |2018-04-25T12:20:45.000Z|2018-04-27          |2018-04-25         |2            |\n",
      "|-2b4qSoMnKE|Ex-UFO program chief: We may not be alone                |17.20.12     |2017-12-19T20:46:33.000Z|2017-12-20          |2017-12-19         |1            |\n",
      "|-37nIo_tLnk|Christmas Day 2000                                       |17.28.12     |2009-12-15T23:26:32.000Z|2017-12-28          |2009-12-15         |2935         |\n",
      "|-37nIo_tLnk|Christmas Day 2000                                       |17.31.12     |2009-12-15T23:26:32.000Z|2017-12-31          |2009-12-15         |2938         |\n",
      "|-3h4Xt9No9o|Why Atlantic fish are invading the Arctic                |18.25.04     |2018-04-23T12:11:34.000Z|2018-04-25          |2018-04-23         |2            |\n",
      "|-3h4Xt9No9o|Why Atlantic fish are invading the Arctic                |18.30.04     |2018-04-23T12:11:34.000Z|2018-04-30          |2018-04-23         |7            |\n",
      "|-4s2MeUgduo|The Weirdest Mystery Tech Yet...                         |18.26.03     |2018-03-20T15:44:52.000Z|2018-03-26          |2018-03-20         |6            |\n",
      "|-4s2MeUgduo|The Weirdest Mystery Tech Yet...                         |18.27.03     |2018-03-20T15:44:52.000Z|2018-03-27          |2018-03-20         |7            |\n",
      "|-4s2MeUgduo|The Weirdest Mystery Tech Yet...                         |18.28.03     |2018-03-20T15:44:52.000Z|2018-03-28          |2018-03-20         |8            |\n",
      "|-4s2MeUgduo|The Weirdest Mystery Tech Yet...                         |18.29.03     |2018-03-20T15:44:52.000Z|2018-03-29          |2018-03-20         |9            |\n",
      "+-----------+---------------------------------------------------------+-------------+------------------------+--------------------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse trending_date (format: YY.DD.MM) and publish_time (ISO format)\n",
    "df_with_dates = df_with_engagement.withColumn(\n",
    "    \"trending_date_parsed\",\n",
    "    F.to_date(F.col(\"trending_date\"), \"yy.dd.MM\")\n",
    ").withColumn(\n",
    "    \"publish_date_parsed\",\n",
    "    F.to_date(F.col(\"publish_time_parsed\"))\n",
    ")\n",
    "\n",
    "# Calculate days_to_trend\n",
    "df_with_days_to_trend = df_with_dates.withColumn(\n",
    "    \"days_to_trend\",\n",
    "    F.when(\n",
    "        F.col(\"trending_date_parsed\").isNull() | F.col(\"publish_date_parsed\").isNull(),\n",
    "        None\n",
    "    ).otherwise(\n",
    "        F.datediff(F.col(\"trending_date_parsed\"), F.col(\"publish_date_parsed\"))\n",
    "    ).cast(IntegerType())\n",
    ")\n",
    "\n",
    "print(\"Days to trend calculated successfully!\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample days_to_trend calculations:\")\n",
    "df_with_days_to_trend.select(\n",
    "    \"video_id\", \"title\", \"trending_date\", \"publish_time\", \n",
    "    \"trending_date_parsed\", \"publish_date_parsed\", \"days_to_trend\"\n",
    ").filter(F.col(\"days_to_trend\").isNotNull()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days to Trend Statistics:\n",
      "count: 40899\n",
      "mean: 16.83\n",
      "stddev: 146.10\n",
      "min: 0\n",
      "max: 4215\n",
      "median: 5\n",
      "\n",
      "Days to Trend Distribution:\n",
      "+-------------+-----+\n",
      "|days_to_trend|count|\n",
      "+-------------+-----+\n",
      "|            0|  121|\n",
      "|            1| 2839|\n",
      "|            2| 4284|\n",
      "|            3| 4639|\n",
      "|            4| 4680|\n",
      "|            5| 4579|\n",
      "|            6| 4038|\n",
      "|            7| 3126|\n",
      "|            8| 2184|\n",
      "|            9| 1657|\n",
      "|           10| 1370|\n",
      "|           11| 1198|\n",
      "|           12| 1082|\n",
      "|           13|  938|\n",
      "|           14|  793|\n",
      "|           15|  608|\n",
      "|           16|  471|\n",
      "|           17|  396|\n",
      "|           18|  312|\n",
      "|           19|  222|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze days_to_trend statistics\n",
    "print(\"Days to Trend Statistics:\")\n",
    "days_stats = df_with_days_to_trend.select(\n",
    "    F.count(\"days_to_trend\").alias(\"count\"),\n",
    "    F.mean(\"days_to_trend\").alias(\"mean\"),\n",
    "    F.stddev(\"days_to_trend\").alias(\"stddev\"),\n",
    "    F.min(\"days_to_trend\").alias(\"min\"),\n",
    "    F.max(\"days_to_trend\").alias(\"max\"),\n",
    "    F.expr(\"percentile_approx(days_to_trend, 0.5)\").alias(\"median\")\n",
    ").collect()[0]\n",
    "\n",
    "for field in days_stats.asDict():\n",
    "    value = days_stats[field]\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{field}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{field}: {value}\")\n",
    "\n",
    "# Show distribution of days_to_trend\n",
    "print(\"\\nDays to Trend Distribution:\")\n",
    "df_with_days_to_trend.groupBy(\"days_to_trend\").count().orderBy(\"days_to_trend\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Calculate Trending Rank\n",
    "\n",
    "Use PySpark Window Functions to rank videos within each `trending_date` and `category_name` based on their `views` in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Calculating trending_rank using Window Functions...\n",
      "Trending rank calculated successfully!\n",
      "\n",
      "Sample trending ranks (Top 3 videos per category per day):\n",
      "+-------------+----------------+-------------+--------------------------------------------------------------------------------------+------------------------------------+--------+\n",
      "|trending_date|category_name   |trending_rank|title                                                                                 |channel_title                       |views   |\n",
      "+-------------+----------------+-------------+--------------------------------------------------------------------------------------+------------------------------------+--------+\n",
      "|17.01.12     |Autos & Vehicles|1            |The Spyker C8 Is the Quirkiest $250,000 Exotic Car in History                         |Doug DeMuro                         |1350802 |\n",
      "|17.01.12     |Autos & Vehicles|2            |I just bought the most ridiculous car                                                 |Simone Giertz                       |633103  |\n",
      "|17.01.12     |Autos & Vehicles|3            |What Jeremy Clarkson thinks about Tesla                                               |Mashable Daily                      |440154  |\n",
      "|17.01.12     |Comedy          |1            |Keeping Up With The Gonzalez’s (Pt. 3) | Lele Pons, Rudy Mancuso & Inanna Sarkis      |Lele Pons                           |5827999 |\n",
      "|17.01.12     |Comedy          |2            |CONFRONTING MY DAD                                                                    |shane                               |3657886 |\n",
      "|17.01.12     |Comedy          |3            |Weird Workout Videos - JonTron                                                        |JonTronShow                         |3457795 |\n",
      "|17.01.12     |Education       |1            |Why Hold Music Sounds Worse Now                                                       |Tom Scott                           |524466  |\n",
      "|17.01.12     |Education       |2            |How a Sick Chimp Led to a Global Pandemic: The Rise of HIV                            |SciShow                             |331220  |\n",
      "|17.01.12     |Education       |3            |Top 5 Programming Languages to Learn to Get a Job at Google, Facebook, Microsoft, etc.|CS Dojo                             |169915  |\n",
      "|17.01.12     |Entertainment   |1            |Marvel Studios' Avengers: Infinity War Official Trailer                               |Marvel Entertainment                |56367282|\n",
      "|17.01.12     |Entertainment   |2            |Pitch Perfect 3 - Riff-Off Clip [HD]                                                  |Pitch Perfect                       |3646907 |\n",
      "|17.01.12     |Entertainment   |3            |Kelly Clarkson Carpool Karaoke                                                        |The Late Late Show with James Corden|3074263 |\n",
      "|17.01.12     |Film & Animation|1            |Love, Simon | Official Trailer [HD] | 20th Century FOX                                |20th Century Fox                    |2096711 |\n",
      "|17.01.12     |Film & Animation|2            |Honest Trailers - The Room                                                            |Screen Junkies                      |1827786 |\n",
      "|17.01.12     |Film & Animation|3            |Everything Wrong With Arrival In 16 Minutes Or Less                                   |CinemaSins                          |1614392 |\n",
      "|17.01.12     |Gaming          |1            |Chuck Norris - Hunter - World of Warcraft TV Commercial - 2011                        |MrNorrisVideos                      |127640  |\n",
      "|17.01.12     |Gaming          |2            |Pokémon Challenge: Watch GAME FREAK’s Kazumasa Iwao Guess the Pokémon!                |The Official Pokémon YouTube Channel|47215   |\n",
      "|17.01.12     |Howto & Style   |1            |Phil is not on fire 9                                                                 |AmazingPhil                         |2127874 |\n",
      "|17.01.12     |Howto & Style   |2            |How to make a Wooden Katana from hardwood flooring // Woodworking                     |I Like To Make Stuff                |1547962 |\n",
      "|17.01.12     |Howto & Style   |3            |We Bought a 100 Year Old Fixer Upper - Man Vs House Ep.1                              |ThreadBanger                        |1344916 |\n",
      "+-------------+----------------+-------------+--------------------------------------------------------------------------------------+------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Calculate trending_rank using Window Functions\n",
    "print(\"Task 3: Calculating trending_rank using Window Functions...\")\n",
    "\n",
    "# Define window specification: partition by trending_date and category_name, order by views descending\n",
    "window_spec = Window.partitionBy(\"trending_date\", \"category_name\").orderBy(F.desc(\"views\"))\n",
    "\n",
    "# Calculate trending_rank\n",
    "df_with_rank = df_with_days_to_trend.withColumn(\n",
    "    \"trending_rank\",\n",
    "    F.row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"Trending rank calculated successfully!\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample trending ranks (Top 3 videos per category per day):\")\n",
    "df_with_rank.select(\n",
    "    \"trending_date\", \"category_name\", \"trending_rank\", \"title\", \"channel_title\", \"views\"\n",
    ").filter(F.col(\"trending_rank\") <= 3).orderBy(\n",
    "    \"trending_date\", \"category_name\", \"trending_rank\"\n",
    ").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trending Rank Analysis:\n",
      "\n",
      "Distribution of trending ranks:\n",
      "+-------------+-----+\n",
      "|trending_rank|count|\n",
      "+-------------+-----+\n",
      "|            1| 2864|\n",
      "|            2| 2625|\n",
      "|            3| 2481|\n",
      "|            4| 2360|\n",
      "|            5| 2215|\n",
      "|            6| 2078|\n",
      "|            7| 1989|\n",
      "|            8| 1897|\n",
      "|            9| 1774|\n",
      "|           10| 1633|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Top-ranked videos (rank = 1) by category and date:\n",
      "Total #1 ranked videos: 2864\n",
      "+-------------+-------------+---------------------------------------------------+-------------------+---------+--------------------+\n",
      "|trending_date|category_name|title                                              |channel_title      |views    |engagement_score    |\n",
      "+-------------+-------------+---------------------------------------------------+-------------------+---------+--------------------+\n",
      "|18.02.06     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|225211923|0.01214679384448043 |\n",
      "|18.01.06     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|220490543|0.012256868540615822|\n",
      "|18.31.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|217750076|0.012340414062587974|\n",
      "|18.30.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|210338856|0.012523225856091943|\n",
      "|18.29.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|205643016|0.012650572582537887|\n",
      "|18.28.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|200820941|0.012787502574245978|\n",
      "|18.27.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|196222618|0.012926195898578827|\n",
      "|18.26.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|190950401|0.013105642024810411|\n",
      "|18.25.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|184446490|0.01332476481390348 |\n",
      "|18.24.05     |Music        |Childish Gambino - This Is America (Official Video)|ChildishGambinoVEVO|179045286|0.013497971122233289|\n",
      "+-------------+-------------+---------------------------------------------------+-------------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze trending rank distribution\n",
    "print(\"Trending Rank Analysis:\")\n",
    "\n",
    "# Count videos by rank position\n",
    "print(\"\\nDistribution of trending ranks:\")\n",
    "df_with_rank.groupBy(\"trending_rank\").count().orderBy(\"trending_rank\").show(10)\n",
    "\n",
    "# Show top-ranked videos across all categories and dates\n",
    "print(\"\\nTop-ranked videos (rank = 1) by category and date:\")\n",
    "top_ranked = df_with_rank.filter(F.col(\"trending_rank\") == 1)\n",
    "print(f\"Total #1 ranked videos: {top_ranked.count()}\")\n",
    "\n",
    "top_ranked.select(\n",
    "    \"trending_date\", \"category_name\", \"title\", \"channel_title\", \"views\", \"engagement_score\"\n",
    ").orderBy(F.desc(\"views\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset with All Features\n",
    "\n",
    "Let's create the final dataset with all the engineered features and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final dataset with all engineered features...\n",
      "Final dataset contains 40,899 records with 14 columns\n",
      "\n",
      "Final dataset schema:\n",
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- likes: long (nullable = true)\n",
      " |-- dislikes: long (nullable = true)\n",
      " |-- comment_count: long (nullable = true)\n",
      " |-- engagement_score: double (nullable = true)\n",
      " |-- days_to_trend: integer (nullable = true)\n",
      " |-- trending_rank: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create final dataset with all features\n",
    "print(\"Creating final dataset with all engineered features...\")\n",
    "\n",
    "# Select relevant columns for the final dataset\n",
    "final_df = df_with_rank.select(\n",
    "    \"video_id\", \"title\", \"channel_title\", \"category_name\", \"country\",\n",
    "    \"trending_date\", \"publish_time\", \"views\", \"likes\", \"dislikes\", \"comment_count\",\n",
    "    \"engagement_score\", \"days_to_trend\", \"trending_rank\"\n",
    ")\n",
    "\n",
    "print(f\"Final dataset contains {final_df.count():,} records with {len(final_df.columns)} columns\")\n",
    "\n",
    "# Show schema of final dataset\n",
    "print(\"\\nFinal dataset schema:\")\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of final dataset with all engineered features:\n",
      "+-----------+-------------------------------------------------------------+------------------+-------------+-------+-------------+------------------------+-------+-------+--------+-------------+-------------------+-------------+-------------+\n",
      "|video_id   |title                                                        |channel_title     |category_name|country|trending_date|publish_time            |views  |likes  |dislikes|comment_count|engagement_score   |days_to_trend|trending_rank|\n",
      "+-----------+-------------------------------------------------------------+------------------+-------------+-------+-------------+------------------------+-------+-------+--------+-------------+-------------------+-------------+-------------+\n",
      "|LsoLEjrDogU|Bruno Mars - Finesse (Remix) [Feat. Cardi B] [Official Video]|Bruno Mars        |Music        |US     |18.04.01     |2018-01-04T04:49:43.000Z|548621 |159356 |2374    |19455        |0.1567371646364248 |0            |15           |\n",
      "|TyHvyGVs42U|Luis Fonsi, Demi Lovato - Échame La Culpa                    |LuisFonsiVEVO     |Music        |US     |17.17.11     |2017-11-17T05:00:01.000Z|499946 |135292 |3528    |12094        |0.14397514931612615|0            |12           |\n",
      "|8O_MwlZ2dEg|j-hope 'Airplane' MV                                         |ibighit           |Music        |US     |18.07.03     |2018-03-06T15:00:10.000Z|5275672|1401915|6268    |158127       |0.14209549039439903|1            |5            |\n",
      "|UaAHl_m_ybk|5 Seconds Of Summer - Want You Back (Audio)                  |5SOSVEVO          |Music        |US     |18.23.02     |2018-02-23T00:00:01.000Z|439056 |106536 |656     |20575        |0.13568132538901642|0            |19           |\n",
      "|9vDdx1dba6c|dodie - Secret For The Mad                                   |dodieVEVO         |Music        |US     |18.27.01     |2018-01-26T18:00:15.000Z|129130 |32755  |98      |2964         |0.133867420429025  |1            |23           |\n",
      "|inZzcTXYowY|Louis Tomlinson - Miss You (Official Video)                  |LouisTomlinsonVEVO|Music        |US     |17.09.12     |2017-12-08T16:00:04.000Z|985998 |241679 |793     |26259        |0.13070594463680454|1            |11           |\n",
      "|D_6QmL6rExk|BTS (방탄소년단) 'FAKE LOVE' Official MV (Extended ver.)     |ibighit           |Music        |US     |18.02.06     |2018-06-01T15:00:01.000Z|5884233|1437859|6390    |134721       |0.12926473169910166|1            |20           |\n",
      "|6mkLWGy6Y3k|Shawn Mendes: The Tour - Official Trailer                    |Shawn Mendes      |Music        |US     |18.10.05     |2018-05-08T17:23:08.000Z|332910 |80118  |232     |5420         |0.12535339881649696|2            |38           |\n",
      "|8O_MwlZ2dEg|j-hope 'Airplane' MV                                         |ibighit           |Music        |US     |18.08.03     |2018-03-06T15:00:10.000Z|6870836|1566591|7910    |167954       |0.12156653135077011|2            |3            |\n",
      "|6mkLWGy6Y3k|Shawn Mendes: The Tour - Official Trailer                    |Shawn Mendes      |Music        |US     |18.11.05     |2018-05-08T17:23:08.000Z|355703 |82016  |249     |5459         |0.12003131826270794|3            |38           |\n",
      "+-----------+-------------------------------------------------------------+------------------+-------------+-------+-------------+------------------------+-------+-------+--------+-------------+-------------------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show comprehensive sample of final dataset\n",
    "print(\"Sample of final dataset with all engineered features:\")\n",
    "final_df.orderBy(F.desc(\"engagement_score\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Engineered Features:\n",
      "==================================================\n",
      "\n",
      "1. Engagement Score:\n",
      "+-----+-------------------+---+------------------+\n",
      "|count|               mean|min|               max|\n",
      "+-----+-------------------+---+------------------+\n",
      "|40899|0.01886712558920193|0.0|0.1567371646364248|\n",
      "+-----+-------------------+---+------------------+\n",
      "\n",
      "\n",
      "2. Days to Trend:\n",
      "+-----+-----------------+---+----+\n",
      "|count|             mean|min| max|\n",
      "+-----+-----------------+---+----+\n",
      "|40899|16.82757524633854|  0|4215|\n",
      "+-----+-----------------+---+----+\n",
      "\n",
      "\n",
      "3. Trending Rank:\n",
      "+-----+------------------+---+---+\n",
      "|count|              mean|min|max|\n",
      "+-----+------------------+---+---+\n",
      "|40899|13.359128585050978|  1| 61|\n",
      "+-----+------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for all engineered features\n",
    "print(\"Summary Statistics for Engineered Features:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Engagement Score\n",
    "print(\"\\n1. Engagement Score:\")\n",
    "final_df.select(\n",
    "    F.count(\"engagement_score\").alias(\"count\"),\n",
    "    F.mean(\"engagement_score\").alias(\"mean\"),\n",
    "    F.min(\"engagement_score\").alias(\"min\"),\n",
    "    F.max(\"engagement_score\").alias(\"max\")\n",
    ").show()\n",
    "\n",
    "# Days to Trend\n",
    "print(\"\\n2. Days to Trend:\")\n",
    "final_df.select(\n",
    "    F.count(\"days_to_trend\").alias(\"count\"),\n",
    "    F.mean(\"days_to_trend\").alias(\"mean\"),\n",
    "    F.min(\"days_to_trend\").alias(\"min\"),\n",
    "    F.max(\"days_to_trend\").alias(\"max\")\n",
    ").show()\n",
    "\n",
    "# Trending Rank\n",
    "print(\"\\n3. Trending Rank:\")\n",
    "final_df.select(\n",
    "    F.count(\"trending_rank\").alias(\"count\"),\n",
    "    F.mean(\"trending_rank\").alias(\"mean\"),\n",
    "    F.min(\"trending_rank\").alias(\"min\"),\n",
    "    F.max(\"trending_rank\").alias(\"max\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "SparkUtils.stop_spark_session()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
